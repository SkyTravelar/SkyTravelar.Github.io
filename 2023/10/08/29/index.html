<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"skytravelar.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="仅供参考">
<meta property="og:type" content="article">
<meta property="og:title" content="中南大学机器学习课程第一次作业">
<meta property="og:url" content="https://skytravelar.github.io/2023/10/08/29/index.html">
<meta property="og:site_name" content="賦幸の個人ブログ">
<meta property="og:description" content="仅供参考">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-10-08T08:21:37.000Z">
<meta property="article:modified_time" content="2023-10-08T08:43:37.388Z">
<meta property="article:author" content="赋 倖">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://skytravelar.github.io/2023/10/08/29/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>中南大学机器学习课程第一次作业 | 賦幸の個人ブログ</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">賦幸の個人ブログ</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://skytravelar.github.io/2023/10/08/29/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="赋 倖">
      <meta itemprop="description" content="东京之北 千叶逢春 好景如水彩">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="賦幸の個人ブログ">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          中南大学机器学习课程第一次作业
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-10-08 16:21:37 / 修改时间：16:43:37" itemprop="dateCreated datePublished" datetime="2023-10-08T16:21:37+08:00">2023-10-08</time>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <div class="post-description">仅供参考</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>描述：这是一个有部分代码缺失的notebook，需要完成的就是将
<code>### START CODE HERE ###</code>
和<code>### END CODE HERE ###</code>之间的代码补全</p>
<h1 id="线性回归">线性回归</h1>
<p>某城市的电网系统需要升级，以应对日益增长的用电需求。电网系统需要考虑最高温度对城市的峰值用电量的影响。项目负责人需要预测明天城市的峰值用电量，他搜集了以往的数据。现在，负责人提供了他搜集到的数据，并请求你帮他训练出一个模型，这个模型能够很好地预测明天城市的峰值用电量。
## 1- 准备 先导入必要的python包</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import time</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<p>导入负责人提供的数据，并可视化数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">data = np.loadtxt(&#x27;data.txt&#x27;)</span><br><span class="line">#data 第一列为温度信息 第二列为人口信息</span><br><span class="line">X_raw = data[:,0].reshape(-1,1)</span><br><span class="line">#data 第三列为用电量信息</span><br><span class="line">Y = data[:,2].reshape(-1,1)</span><br><span class="line">plt.xlabel(&#x27;High temperature&#x27;)</span><br><span class="line">plt.ylabel(&#x27;Peak demand &#x27;)</span><br><span class="line">plt.scatter(X_raw,Y)</span><br><span class="line">print(&#x27;X shape:&#x27;,X_raw.shape)</span><br><span class="line">print(&#x27;Y shape:&#x27;,Y.shape)</span><br><span class="line">print(&#x27;some X[:5]:\n&#x27;,X_raw[:5])</span><br><span class="line">print(&#x27;some Y[:5]:\n&#x27;,Y[:5])</span><br></pre></td></tr></table></figure>
<p>结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">X shape: (80, 1)</span><br><span class="line">Y shape: (80, 1)</span><br><span class="line">some X[:5]:</span><br><span class="line"> [[38.24]</span><br><span class="line"> [36.53]</span><br><span class="line"> [32.92]</span><br><span class="line"> [26.59]</span><br><span class="line"> [20.05]]</span><br><span class="line">some Y[:5]:</span><br><span class="line"> [[4.04]</span><br><span class="line"> [2.84]</span><br><span class="line"> [3.2 ]</span><br><span class="line"> [3.42]</span><br><span class="line"> [2.32]]</span><br></pre></td></tr></table></figure>
<p>根据对数据可视化结果的分析，决定使用回归算法训练一个模型，用来预测明天城市的峰值用电量。首先考虑单变量的线性回归模型。</p>
<h2 id="单变量线性回归理论介绍">2- 单变量线性回归理论介绍</h2>
<h3 id="单变量线性回归模型">单变量线性回归模型</h3>
<p>单变量线性回归的模型由两个参数<span
class="math inline">\(\theta_0\)</span>,<span
class="math inline">\(\theta_1\)</span>来表示一条直线：<span
class="math display">\[Peak\ demand \approx \theta_0 + \theta_1 \cdot
(High\ temperature) 。\]</span></p>
<p>我们的目标也就是找到一条"最符合"的直线，确定这条直线的参数<span
class="math inline">\(\theta_i\)</span>。</p>
<p>设输入的特征——最高温度(F)为<span class="math inline">\(x^{(i)} \in
\mathbb{R}^{n+1}\)</span>，<span
class="math inline">\(i=1,\cdots,m\)</span>。<span
class="math inline">\(m\)</span>为样本总数，在该例子中<span
class="math inline">\(m\)</span>=80。<span
class="math inline">\(n\)</span>为特征的个数，这里为<span
class="math inline">\(1\)</span>。则：<span
class="math inline">\(x^{(i)} \in \mathbb{R}^2 = \begin{bmatrix} 1 \\
\text{high temperature for day} i\end{bmatrix}。\)</span></p>
<p>设输出为<span class="math inline">\(y^{(i)} \in
\mathbb{R}\)</span>，表示第<span
class="math inline">\(i\)</span>天的峰值用电量。</p>
<p>参数为<span class="math inline">\(\theta \in \mathbb{R}^{n+1} =
\begin{bmatrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_n
\end{bmatrix}\)</span>。这里<span
class="math inline">\(n=1\)</span>。</p>
<p>在该例子中，模型为一条直线，模型可表示为： <span
class="math display">\[h_{\theta}(x) = \theta^T x = \theta_0 + \theta_1
x 。\]</span> ### <strong>注意</strong>： 这里的<span
class="math inline">\(\theta^T\)</span>是一个向量，<span
class="math inline">\(\theta_0,\theta_1\)</span>是标量。使用向量化表示的原因为：（1）简化数学公式的书写（2）与程序代码中的表示保持一致，且使用向量化的代码实现可以加速运算，<strong>因此一般能不用<code>for</code>循环的地方都不用<code>for</code>循环</strong>。</p>
<p>下面用一个简单的例子说明向量化的代码运算更快。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 随机初始化两个向量，计算它们的点积</span><br><span class="line">x = np.random.rand(10000000,1)</span><br><span class="line">y = np.random.rand(10000000,1)</span><br><span class="line">ans = 0</span><br><span class="line">start = time.time()</span><br><span class="line">for i in range(10000000):</span><br><span class="line">    ans += x[i,0]/y[i,0]</span><br><span class="line">end = time.time()</span><br><span class="line">print(&#x27;for循环的计算时间: %.2fs&#x27;%(end - start))</span><br><span class="line">print(&#x27;计算结果：%.2f&#x27;%(ans))</span><br><span class="line">start = time.time()</span><br><span class="line">ans = np.dot(x.T,y)</span><br><span class="line">end = time.time()</span><br><span class="line">print(&#x27;向量化的计算时间: %.2fs&#x27;%(end - start))</span><br><span class="line">print(&#x27;计算结果：%.2f&#x27;%(ans))</span><br></pre></td></tr></table></figure>
<p>结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for循环的计算时间: 4.67s</span><br><span class="line">计算结果：89066284.16</span><br><span class="line">向量化的计算时间: 0.00s</span><br><span class="line">计算结果：2500035.03</span><br></pre></td></tr></table></figure>
<p>因为<span class="math inline">\(\theta_0 + \theta_1 x=\begin{bmatrix}
1 \quad x \end{bmatrix} \begin{bmatrix} \theta_0 \\ \theta_1
\end{bmatrix} 。\)</span> 因此，为了方便编程，我们需要给每一个<span
class="math inline">\(x^{(i)}\)</span>的前面再加一列1。使得每一个<span
class="math inline">\(x^{(i)}\)</span>成为一个2维向量。</p>
<h3 id="预测结果">预测结果</h3>
<p>模型需要根据输入自变量 <span class="math inline">\(x^{(i)}\)</span>
和参数 <span class="math inline">\(\theta\)</span> 来输出预测结果 <span
class="math inline">\(predict\_y^{(i)}\)</span>。</p>
<p>将自变量 <span class="math inline">\(x^{(i)}\)</span>
作为模型的输入，模型根据输入和当前参数 <span
class="math inline">\(\theta\)</span> 输出预测结果：</p>
<p><span class="math display">\[
predict\_y^{(i)} = h_\theta(x^{(i)})。
\]</span></p>
<p>其中 <span class="math inline">\(h_\theta()\)</span> 为模型在参数为
<span class="math inline">\(\theta\)</span>
情况下，对于输入的预测函数。</p>
<p>在预测阶段，<span class="math inline">\(x\)</span>作为自变量。 ###
损失函数</p>
<p>模型的预测结果和实际结果有差距，为了衡量它们之间的差距，或者说量化使用这个模型产生的损失，我们定义损失函数<span
class="math inline">\(l(predict\_y^{(i)},
y^{(i)})\)</span>。这里我们使用平方损失： <span class="math display">\[
l(predict\_y, y) = \left ( predict\_y^{(i)} - y^{(i)} \right )^2。
\]</span></p>
<p>上述损失函数表示一个样本的损失，整个训练集的损失使用<span
class="math inline">\(J(\theta)\)</span>表示： <span
class="math display">\[
\begin{aligned}
J(\theta) &amp; = \frac{1}{2m} \sum_{i=1}^{m}l(predict\_y^{(i)},
y^{(i)}) \\
&amp; = \frac{1}{2m} \sum_{i=1}^{m} \left ( h_\theta(x^{(i)}) - y^{(i)}
\right )^2 \\
&amp; = \frac{1}{2m} \sum_{i=1}^{m} \left ( \theta^T x^{(i)} - y^{(i)}
\right )^2。
\end{aligned}
\]</span> （其中数字2的作用是方便求导时的运算）</p>
<p>为了使模型取得较好的预测效果，需要最小化训练集上的损失，即<span
class="math inline">\(\underset{\theta}{\min} J(\theta)\)</span>。</p>
<p>在损失阶段，<span class="math inline">\(\theta\)</span>
作为自变量。</p>
<h3 id="梯度下降法">梯度下降法</h3>
<p>为了得到使损失函数<span
class="math inline">\(J(\theta)\)</span>最小化的<span
class="math inline">\(\theta\)</span>，可以使用梯度下降法。</p>
<p>假设一开始<span
class="math inline">\(J(\theta)\)</span>的值在紫色点上，为了降低<span
class="math inline">\(J(\theta)\)</span>值，需要<span
class="math inline">\(\theta_1\)</span>往右边移动，这个方向是<span
class="math inline">\(J(\theta)\)</span>在<span
class="math inline">\(\theta_1\)</span>上的负梯度。只要<span
class="math inline">\(\theta\)</span>不断往负梯度方向移动，<span
class="math inline">\(J(\theta)\)</span>一定可以降到最低值。梯度下降法就是使参数<span
class="math inline">\(\theta\)</span>不断往负梯度移动，经过有限次迭代(更新<span
class="math inline">\(\theta\)</span>值)之后，损失函数<span
class="math inline">\(J(\theta)\)</span>达到最低值。</p>
<p>梯度下降法的过程： 1. 初始化参数向量<span
class="math inline">\(\theta\)</span>。</p>
<ol start="2" type="1">
<li><p>开始迭代</p>
<p>A.根据实际输入<span class="math inline">\(x\)</span>和参数<span
class="math inline">\(\theta\)</span>预测输出，</p>
<p>B. 根据预测输出值和实际输出值之间的差距，计算损失函数<span
class="math inline">\(J(\theta)\)</span>，</p>
<p>C. 计算损失函数对<span
class="math inline">\(\theta\)</span>的梯度，</p>
<p>D. 更新参数<span class="math inline">\(\theta\)</span>。</p></li>
</ol>
<h1 id="实现单变量线性回归模型">3- 实现单变量线性回归模型</h1>
<p>现在，我们开始实现 Regression 算法。</p>
<h3 id="任务1"><strong>任务1：</strong></h3>
<p>首先在<span
class="math inline">\(X\)</span>前面加上一列1，表示参数<span
class="math inline">\(\theta_0\)</span>的系数，方便运算。<span
class="math inline">\(X\)</span>是形状为<span
class="math inline">\((m,1)\)</span>的矩阵，一共<span
class="math inline">\(m\)</span>行数据，我们需要为每一行数据的前面加一列1，如下所示：<br />
<span class="math display">\[
\begin{bmatrix} x^{(0)} \\ x^{(1)} \\ \vdots \\x^{(m-1)}  \end{bmatrix}
\longrightarrow
\begin{bmatrix} 1\quad x^{(0)} \\ 1\quad x^{(1)} \\ \vdots \\ 1\
x^{(m-1)}  \end{bmatrix}。
\]</span>
<strong>提示</strong>：可以使用<code>np.hstack</code>把两个矩阵水平合在一起。用1初始化向量或矩阵的函数是<code>np.ones</code>。(函数详情可使用python的帮助函数<code>help</code>，比如<code>help(np.ones)</code>，或者自行用搜索引擎检索。)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def preprocess_data(X):</span><br><span class="line">    &quot;&quot;&quot;输入预处理 在X前面加一列1</span><br><span class="line">    参数：</span><br><span class="line">        X:原始数据,shape为(m,1)</span><br><span class="line">	返回：</span><br><span class="line">    	X_train: 在X加一列1的数据,shape为(m,2)</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">m = X.shape[0]   # m 是数据X的行数</span><br><span class="line"></span><br><span class="line">### START CODE HERE ###</span><br><span class="line"></span><br><span class="line">temp=np.ones((m,1))</span><br><span class="line">X_train = np.hstack((temp,X))</span><br><span class="line"></span><br><span class="line">### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">return X_train</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = preprocess_data(X_raw)</span><br><span class="line">print(&#x27;new X shape:&#x27;,X.shape)</span><br><span class="line">print(&#x27;Y shape:&#x27;,Y.shape)</span><br><span class="line">print(&#x27;new X[:5,:]=\n&#x27;,X[:5,:])</span><br><span class="line">print(&#x27;Y[:5,:]=\n&#x27;,Y[:5,:])</span><br></pre></td></tr></table></figure>
<p>结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">new X shape: (80, 2)</span><br><span class="line">Y shape: (80, 1)</span><br><span class="line">new X[:5,:]=</span><br><span class="line"> [[ 1.   38.24]</span><br><span class="line"> [ 1.   36.53]</span><br><span class="line"> [ 1.   32.92]</span><br><span class="line"> [ 1.   26.59]</span><br><span class="line"> [ 1.   20.05]]</span><br><span class="line">Y[:5,:]=</span><br><span class="line"> [[4.04]</span><br><span class="line"> [2.84]</span><br><span class="line"> [3.2 ]</span><br><span class="line"> [3.42]</span><br><span class="line"> [2.32]]</span><br></pre></td></tr></table></figure>
<h3 id="任务2"><strong>任务2：</strong></h3>
<p>接着，初始化参数向量<span
class="math inline">\(\theta\)</span>。<span
class="math inline">\(\theta\)</span>的shape是<span
class="math inline">\((2,1)\)</span>，我们随机初始化<span
class="math inline">\(\theta\)</span>。</p>
<p><strong>提示</strong>：numpy的随机函数是<code>np.random.rand</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def init_parameter(shape):</span><br><span class="line">    &quot;&quot;&quot;初始化参数</span><br><span class="line">    参数：</span><br><span class="line">        shape: 参数形状</span><br><span class="line">	返回：</span><br><span class="line">    	theta_init: 初始化后的参数</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">np.random.seed(0)</span><br><span class="line">m, n = shape</span><br><span class="line"></span><br><span class="line">### START CODE HERE ###</span><br><span class="line"></span><br><span class="line">theta_init = np.random.rand(m,n)</span><br><span class="line"></span><br><span class="line">### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">return theta_ini</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">theta = init_parameter((2,1))</span><br><span class="line">print(&#x27;theta shape is &#x27;,theta.shape)</span><br><span class="line">print(&#x27;theta = &#x27;,theta)</span><br></pre></td></tr></table></figure>
<p>结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">theta shape is  (2, 1)</span><br><span class="line">theta =  [[0.5488135 ]</span><br><span class="line"> [0.71518937]]</span><br></pre></td></tr></table></figure>
<h3 id="任务3"><strong>任务3：</strong></h3>
<p>通过已知 <span class="math inline">\(X\)</span> 和参数 <span
class="math inline">\(\theta\)</span> 计算预测的 <span
class="math inline">\(predict\_Y\)</span> 值。</p>
<p>由于使用<code>for</code>循环单独计算每个预测值效率不高，因此我们需要用向量化的方法代替<code>for</code>循环。<span
class="math inline">\(X\)</span> 大小为<span class="math inline">\(m
\times (n+1)\)</span>(<span
class="math inline">\(n\)</span>表示特征数量，这里<span
class="math inline">\(n=1\)</span>)，每行是一条样本特征向量，<span
class="math inline">\(\theta\)</span> 大小为<span
class="math inline">\((n+1) \times 1\)</span>，可以使用<span
class="math inline">\(X
\theta\)</span>（矩阵相乘）计算所有样本的预测结果,大小为<span
class="math inline">\(m\times
1\)</span>。于是这里的线性模型就可以表示为： <span
class="math display">\[
h_{\theta}(X) = X \theta。
\]</span> 这里<span
class="math inline">\(h_{\theta}(X)\)</span>的大小为<span
class="math inline">\(m \times 1\)</span>，结果上等于 <span
class="math inline">\(predict\_Y_\theta\)</span>。</p>
<p><strong>提示</strong>：矩阵相乘
<code>np.dot(矩阵1，矩阵2)</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def compute_predict_Y(X,theta):</span><br><span class="line">    &quot;&quot;&quot;计算预测结果</span><br><span class="line">    参数：</span><br><span class="line">        X: 训练集数据特征,shape: (m, 2)</span><br><span class="line">        theta: 参数,shape: (2, 1)</span><br><span class="line">	返回：</span><br><span class="line">    	predict_Y: 预测结果,shape: (m,1)</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">### START CODE HERE ###</span><br><span class="line"></span><br><span class="line">predict_Y = np.dot(X,theta)</span><br><span class="line"></span><br><span class="line">### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">return predict_Y</span><br><span class="line"></span><br><span class="line">predict_Y = compute_predict_Y(X,theta)</span><br><span class="line">print(predict_Y[:5])</span><br></pre></td></tr></table></figure>
<p>结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[[27.89765487]</span><br><span class="line"> [26.67468106]</span><br><span class="line"> [24.09284744]</span><br><span class="line"> [19.56569876]</span><br><span class="line"> [14.8883603 ]]</span><br></pre></td></tr></table></figure>
<h3 id="任务4"><strong>任务4：</strong></h3>
<p>实现计算损失函数<span
class="math inline">\(J(\theta)\)</span>的函数。<br />
从公式 <span class="math display">\[
\begin{aligned}
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \left ( predict\_y_\theta^{(i)}
- y_\theta^{(i)} \right )^2
\end{aligned}
\]</span>
可以看到有个求和，由于使用<code>for</code>循环效率不高，因此需要用向量化的方法代替<code>for</code>循环。<span
class="math inline">\((predict\_Y -
Y)^2\)</span>计算所有样本的损失值，最后求和并除以<span
class="math inline">\(2m\)</span>得到<span
class="math inline">\(J(\theta)\)</span>的值，得到的<span
class="math inline">\(J(\theta)\)</span>是一个标量。<br />
<strong>提示</strong>：矩阵乘法运算可使用<code>np.dot</code>函数，平方运算可使用<code>np.power(data, 2)</code>函数，求和运算可使用<code>np.sum</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def compute_J(predict_Y, Y):</span><br><span class="line">    &quot;&quot;&quot;计算损失的函数J</span><br><span class="line">    参数：</span><br><span class="line">        predict_Y: 预测结果,shape: (m, 1)</span><br><span class="line">        Y: 训练集数据标签,shape: (m, 1)</span><br><span class="line">	返回：</span><br><span class="line">    	loss: 损失值</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">m = Y.shape[0]</span><br><span class="line"></span><br><span class="line">### START CODE HERE ###</span><br><span class="line"></span><br><span class="line">loss = np.sum(np.power((predict_Y-Y),2))/(2 * m)</span><br><span class="line"></span><br><span class="line">### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">return loss</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">first_loss = compute_J(predict_Y, Y)</span><br><span class="line">print(&quot;first_loss = &quot;, first_loss)</span><br></pre></td></tr></table></figure>
<p>结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">first_loss =  144.05159786255672</span><br></pre></td></tr></table></figure>
<h3 id="任务5"><strong>任务5：</strong></h3>
<p>计算参数<span
class="math inline">\(\theta\)</span>的梯度。梯度计算的公式为： <span
class="math display">\[
\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m}
\sum_{i=1}^{m} \left ( \theta^T x^{(i)} - y \right ) x_j^{(i)}。
\]</span> 向量化公式为： <span class="math display">\[
\text{gradients} =\frac{1}{m} X^T (X \theta - Y) 。
\]</span>
<strong>提示</strong>：矩阵A的转置表示为<code>A.T</code>。<span
class="math inline">\(X\theta\)</span>就是计算出的predict_Y。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def compute_gradient(predict_Y, Y, X):</span><br><span class="line">    &quot;&quot;&quot;计算对参数theta的梯度值</span><br><span class="line">    参数：</span><br><span class="line">        predict_Y: 当前预测结果,shape: (m,1)</span><br><span class="line">        Y: 训练集数据标签,shape: (m, 1)</span><br><span class="line">        X: 训练集数据特征,shape: (m, 2)</span><br><span class="line">	返回：</span><br><span class="line">    	gradients: 对theta的梯度,shape:(2,1)</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">m = X.shape[0]</span><br><span class="line"></span><br><span class="line">### START CODE HERE ###</span><br><span class="line"></span><br><span class="line">gradients = np.dot(X.T,predict_Y-Y) / m</span><br><span class="line"></span><br><span class="line">### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">return gradients</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gradients_first = compute_gradient(predict_Y, Y, X)</span><br><span class="line">print(&quot;gradients_first shape : &quot;, gradients_first.shape)</span><br><span class="line">print(&quot;gradients_first = &quot;, gradients_first)</span><br></pre></td></tr></table></figure>
<p>结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gradients_first shape :  (2, 1)</span><br><span class="line">gradients_first =  [[ 16.0079445 ]</span><br><span class="line"> [459.96770081]]</span><br></pre></td></tr></table></figure>
<h3 id="任务6"><strong>任务6：</strong></h3>
<p>用梯度下降法更新参数<span
class="math inline">\(\theta\)</span>,实现<code>update_parameters</code>函数。</p>
<p><strong>提示</strong>：parameters = <span
class="math inline">\(\theta\)</span> - <span
class="math inline">\(learning\_rate·gradients\)</span></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def update_parameters(theta, gradients, learning_rate=0.0001):</span><br><span class="line">    &quot;&quot;&quot;更新参数theta</span><br><span class="line">    参数：</span><br><span class="line">        theta: 参数,shape: (2, 1)</span><br><span class="line">        gradients: 梯度,shape: (2, 1)</span><br><span class="line">        learning_rate: 学习率,默认为0.0001</span><br><span class="line">	返回：</span><br><span class="line">    	parameters: 更新后的参数,shape: (2, 1)</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">### START CODE HERE ###</span><br><span class="line">parameters = theta - learning_rate * gradients</span><br><span class="line">### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">return parameters</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">theta_one_iter = update_parameters(theta, gradients_first)</span><br><span class="line"></span><br><span class="line">print(&quot;theta_one_iter = &quot;, theta_one_iter)</span><br></pre></td></tr></table></figure>
<p>结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">theta_one_iter =  [[0.54721271]</span><br><span class="line"> [0.6691926 ]]</span><br></pre></td></tr></table></figure>
<h3 id="任务7"><strong>任务7：</strong></h3>
<p>将前面定义的函数整合起来，实现完整的模型训练函数。</p>
<p><span class="math inline">\(\theta\)</span>迭代更新
<code>iter_num</code>次。迭代次数<code>iter_num</code>也是一个超参数，如果<code>iter_num</code>太小，损失函数<span
class="math inline">\(J(\theta)\)</span>还没有收敛；如果<code>iter_num</code>太大，损失函数<span
class="math inline">\(J(\theta)\)</span>早就收敛了，过多的迭代浪费时间。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">def model(X, Y, theta, iter_num = 100, learning_rate=0.0001):</span><br><span class="line">    &quot;&quot;&quot;线性回归模型</span><br><span class="line">    参数：</span><br><span class="line">        X: 训练集数据特征,shape: (m, n+1)</span><br><span class="line">        Y: 训练集数据标签,shape: (m, 1)</span><br><span class="line">        iter_num: 梯度下降的迭代次数</span><br><span class="line">        theta: 初始化的参数,shape: (n+1, 1)</span><br><span class="line">        learning_rate: 学习率,默认为0.0001</span><br><span class="line">    返回：</span><br><span class="line">        loss_history: 每次迭代的损失值</span><br><span class="line">        theta_history: 每次迭代更新后的参数</span><br><span class="line">        theta: 训练得到的参数</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">loss_history = []</span><br><span class="line">theta_history = []</span><br><span class="line"></span><br><span class="line">for i in range(iter_num):</span><br><span class="line">    ### START CODE HERE ###</span><br><span class="line">    # 预测</span><br><span class="line">​    predict_Y = compute_predict_Y(X,theta)</span><br><span class="line">    # 计算损失</span><br><span class="line">​    loss = compute_J(predict_Y,Y)</span><br><span class="line">    # 计算梯度</span><br><span class="line">​    gradients = compute_gradient(predict_Y,Y,X)</span><br><span class="line">    # 更新参数</span><br><span class="line">​    theta = update_parameters(theta,gradients,learning_rate)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">​    loss_history.append(loss)</span><br><span class="line">​    theta_history.append(theta)</span><br><span class="line"></span><br><span class="line">return loss_history, theta_history, theta</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 感兴趣的同学可以自行尝试不同的学习率和迭代次数，最后提交时以100次迭代和0.0001的学习率重新运行一遍再提交</span><br><span class="line"></span><br><span class="line">loss_history, theta_history, theta = model(X, Y, theta, iter_num=100, learning_rate=0.0001)</span><br><span class="line"></span><br><span class="line">print(&quot;theta = &quot;, theta)</span><br><span class="line"></span><br><span class="line">plt.plot(loss_history)</span><br><span class="line">print(&quot;loss = &quot;, loss_history[-1])</span><br></pre></td></tr></table></figure>
<p>下面是学习到的线性模型与原始数据的关系可视化。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X[:,1],Y)</span><br><span class="line">x = np.arange(10,42)</span><br><span class="line">plt.plot(x,x*theta[1][0]+theta[0][0],&#x27;r&#x27;)</span><br></pre></td></tr></table></figure>
<p>现在直观地了解一下梯度下降的过程。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">theta_0 = np.linspace(0, 1, 50)</span><br><span class="line">theta_1 = np.linspace(0, 1, 50)</span><br><span class="line">theta_0, theta_1 = np.meshgrid(theta_0,theta_1)</span><br><span class="line">J = np.zeros_like(theta_0)</span><br><span class="line">predict_Ys = np.zeros_like(predict_Y)</span><br><span class="line">print(theta_0.shape)</span><br><span class="line">print(theta_1.shape)</span><br><span class="line">print(predict_Ys.shape)</span><br><span class="line">print(J.shape)</span><br><span class="line"></span><br><span class="line">for i in range(50):</span><br><span class="line">    for j in range(50):</span><br><span class="line">        predict_Y = compute_predict_Y(X, np.array([[theta_0[i,j]],[theta_1[i,j]]]))</span><br><span class="line">        J[i,j] = compute_J(predict_Y, Y)</span><br><span class="line"></span><br><span class="line">plt.contourf(theta_0, theta_1, J, 10, alpha = 0.6, cmap = plt.cm.coolwarm)</span><br><span class="line">C = plt.contour(theta_0, theta_1, J, 10, colors = &#x27;black&#x27;)</span><br><span class="line"></span><br><span class="line"># 画出损失函数J的历史位置</span><br><span class="line"></span><br><span class="line">history_num = len(theta_history)</span><br><span class="line">theta_0_history = np.zeros(history_num)</span><br><span class="line">theta_1_history = np.zeros(history_num)</span><br><span class="line">for i in range(history_num):</span><br><span class="line">    theta_0_history[i],theta_1_history[i] = theta_history[i][0,0],theta_history[i][1,0]</span><br><span class="line">plt.scatter(theta_0_history, theta_1_history, c=&quot;r&quot;)</span><br></pre></td></tr></table></figure>
<p>可以看到，<span
class="math inline">\(J(\theta)\)</span>的值不断地往最低点移动。在y轴，<span
class="math inline">\(J(\theta)\)</span>下降的比较快，在x轴，<span
class="math inline">\(J(\theta)\)</span>下降的比较慢。</p>
<h2 id="实现多变量线性回归模型">4- 实现多变量线性回归模型</h2>
<p>上述例子是单变量回归的例子，样本的特征只有一个一天的最高温度。负责人经过分析后发现，城市一天的峰值用电量还与城市人口有关系，因此，他在回归模型中添加城市人口变量<span
class="math inline">\(x_2\)</span>，你的任务是训练这个多变量回归方程：
<span class="math display">\[
h(x) = \theta^T x = \theta_0 * 1 + \theta_1 * x_1 + \theta_2 * x_2。
\]</span> 之前实现的梯度下降法使用的对象是<span
class="math inline">\(\theta\)</span>和<span
class="math inline">\(X\)</span>向量，实现的梯度下降函数适用于单变量回归和多变量回归。不难发现上面使用的向量化公式在多变量回归里依然不变，因此代码也基本一致,直接调用前面实现的函数即可。</p>
<h3 id="任务8"><strong>任务8：</strong></h3>
<p>现在，训练一个多变量回归模型。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#读取数据，X取data的前两列</span><br><span class="line">X = data[:,0:2].reshape(-1, 2)</span><br><span class="line">Y = data[:,2].reshape(-1, 1)</span><br><span class="line"></span><br><span class="line">### START CODE HERE ###</span><br><span class="line"># 直接调用上面实现过的函数</span><br><span class="line"># 同样为X的前面添加一列1,使得X的shape从80x2 -&gt; 80x3</span><br><span class="line">X = preprocess_data(X)</span><br><span class="line"># 初始化参数theta ,theta的shape应为 3x1</span><br><span class="line">theta = init_parameter((3,1))</span><br><span class="line"># 传入模型训练,learning_rate设为0.0001</span><br><span class="line">loss_history, theta_history, theta = model(X,Y,theta,iter_num=100,learning_rate=0.0001)</span><br><span class="line">### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">print(&quot;theta = &quot;, theta)</span><br><span class="line"></span><br><span class="line">plt.plot(loss_history)</span><br><span class="line">print(&quot;loss = &quot;, loss_history[-1])</span><br></pre></td></tr></table></figure>
<p>结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">theta =  [[0.52593585]</span><br><span class="line"> [0.06715361]</span><br><span class="line"> [0.57583208]]</span><br><span class="line">loss =  0.10300473270580184</span><br></pre></td></tr></table></figure>
<h1 id="特征归一化">5- 特征归一化</h1>
<p>特征归一化可以确保特征在相同的尺度，加快梯度下降的收敛过程。</p>
<h3 id="任务9"><strong>任务9：</strong></h3>
<p>对数据进行零均值单位方差归一化处理。零均值单位方差归一化公式： <span
class="math display">\[
x_i = \frac{x_i - \mu_i}{\sigma_i}
\]</span> 其中<span class="math inline">\(i\)</span>表示第<span
class="math inline">\(i\)</span>个特征，<span
class="math inline">\(\mu_i\)</span>表示第<span
class="math inline">\(i\)</span>个特征的均值，<span
class="math inline">\(\sigma_i\)</span>表示第<span
class="math inline">\(i\)</span>个特征的标准差。进行零均值单位方差归一化处理后，数据符合标准正态分布，即均值为0，标准差为1。</p>
<p><strong>注意</strong>，使用新样本进行预测时，需要对样本的特征进行相同的缩放处理。</p>
<p><strong>提示</strong>：求特征的均值，使用numpy的函数<code>np.mean</code>;求特征的标准差，使用numpy的函数<code>np.std</code>，需要注意对哪个维度求均值和标准差。比如，对矩阵A的每列求均值<code>np.mean(A,axis=0)</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">X = data[:,0:2].reshape((-1, 2))</span><br><span class="line">Y = data[:,2].reshape((-1, 1))</span><br><span class="line"></span><br><span class="line">### START CODE HERE ###</span><br><span class="line"># 计算特征的均值 mu</span><br><span class="line">mu = np.mean( X , axis=0 )</span><br><span class="line"># 计算特征的标准差 sigma</span><br><span class="line">sigma = np.std( X , axis=0 )</span><br><span class="line"># 零均值单位方差归一化</span><br><span class="line">X_norm = ( X - mu ) / sigma</span><br><span class="line"># 训练多变量回归模型</span><br><span class="line"># X_norm前面加一列1</span><br><span class="line">X = preprocess_data(X_norm)</span><br><span class="line"># 初始化参数theta</span><br><span class="line">theta = init_parameter((3,1))</span><br><span class="line"># 传入模型训练,learning_rate设为0.1</span><br><span class="line">loss_history, theta_history, theta = model(X,Y,theta,iter_num = 100, learning_rate=0.1)</span><br><span class="line">### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">print(&quot;mu = &quot;, mu)</span><br><span class="line">print(&quot;sigma = &quot;, sigma)</span><br><span class="line"></span><br><span class="line">print(&quot;theta = &quot;, theta)</span><br><span class="line"></span><br><span class="line">plt.plot(loss_history)</span><br><span class="line">print(&quot;loss = &quot;, loss_history[-1])</span><br></pre></td></tr></table></figure>
<p>我们来直观地了解特征尺度归一化的梯度下降的过程。这里只展示单变量回归梯度下降过程。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">X_show = X[:,0:2]</span><br><span class="line">X_show = preprocess_data(X_show)</span><br><span class="line"></span><br><span class="line">theta_0 = np.linspace(-2, 3, 50)</span><br><span class="line">theta_1 = np.linspace(-2, 3, 50)</span><br><span class="line">theta_0, theta_1 = np.meshgrid(theta_0,theta_1)</span><br><span class="line">J = np.zeros_like(theta_0)</span><br><span class="line"></span><br><span class="line">for i in range(50):</span><br><span class="line">    for j in range(50):</span><br><span class="line">        predict_Y = compute_predict_Y(X_show, np.array([[2.877],[theta_0[i,j]],[theta_1[i,j]]]))</span><br><span class="line">        J[i,j] = compute_J(predict_Y, Y)</span><br><span class="line"></span><br><span class="line">plt.contourf(theta_0, theta_1, J, 10, alpha = 0.6, cmap = plt.cm.coolwarm)</span><br><span class="line">C = plt.contour(theta_0, theta_1, J, 10, colors = &#x27;black&#x27;)</span><br><span class="line"></span><br><span class="line"># 画出损失函数J的历史位置</span><br><span class="line"></span><br><span class="line">history_num = len(theta_history)</span><br><span class="line">theta_0_history = np.zeros(history_num)</span><br><span class="line">theta_1_history = np.zeros(history_num)</span><br><span class="line">for i in range(history_num):</span><br><span class="line">    theta_0_history[i],theta_1_history[i] = theta_history[i][2,0],theta_history[i][1,0]</span><br><span class="line">plt.scatter(theta_0_history, theta_1_history, c=&quot;r&quot;)</span><br></pre></td></tr></table></figure>
<p>可以看到，<span
class="math inline">\(J(\theta)\)</span>的值不断地往最低点移动。与没有进行特征尺度归一化的图相比，归一化后，每个维度的变化幅度大致相同，这有助于<span
class="math inline">\(J(\theta)\)</span>的值快速下降到最低点。</p>
<h1 id="法线方程-the-normal-equations">6- 法线方程 (The normal
equations)</h1>
<p>对于求函数极小值问题，可以使用求导数的方法，令函数的导数为0，然后求解方程，得到解析解。法线方程正是使用这种方法来求解损失函数<span
class="math inline">\(J(\theta)\)</span>的极小值，而线性回归的损失函数<span
class="math inline">\(J(\theta)\)</span>是一个凸函数，所以极小值就是最小值。</p>
<p>法线方程的求解过程详见课件，法线方程的公式是： <span
class="math display">\[
\theta = (X^T X)^{-1} X^T Y
\]</span></p>
<p>如果<span class="math inline">\(m \le n +1\)</span>，那么<span
class="math inline">\(X^T X\)</span>是奇异矩阵，即<span
class="math inline">\(X^T X\)</span>不可逆。 <span
class="math inline">\(X^T X\)</span>不可逆的原因可能是：</p>
<ul>
<li>特征之间冗余，比如特征向量中两个特征是线性相关的。</li>
<li>特征太多，删去一些特征再进行运算。</li>
</ul>
<p>法线方程的缺点之一就是会出现<span class="math inline">\(X^T
X\)</span>不可逆的情况，可以通过正则化的方式解决。另一个缺点是，如果样本的个数太多，特征数量太多(<span
class="math inline">\(n \gt
10000\)</span>)，法线方程的运算会很慢（求逆矩阵的运算复杂）。</p>
<h3 id="任务10"><strong>任务10：</strong></h3>
<p>下面来实现法线方程。<br />
<strong>提示</strong>：Numpy
求逆矩阵的函数是<code>np.linalg.inv</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def normal_equation(X, Y):</span><br><span class="line">    &quot;&quot;&quot;法线方程求解线性回归方程的参数</span><br><span class="line">    参数：</span><br><span class="line">        X: 训练集数据特征,shape: (m, n+1)</span><br><span class="line">        Y: 训练集数据标签,shape: (m, 1)</span><br><span class="line">    返回：</span><br><span class="line">        theta: 线性回归方程的参数</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">### START CODE HERE ###</span><br><span class="line">theta = np.dot(np.dot(np.linalg.inv(np.dot(X.T,X)),X.T),Y)</span><br><span class="line">### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">return theta</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">theta = normal_equation(X, Y)</span><br><span class="line"></span><br><span class="line">print(&quot;theta = &quot;, theta)</span><br></pre></td></tr></table></figure>
<p>结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">theta =  [[2.861875  ]</span><br><span class="line"> [0.70429906]</span><br><span class="line"> [0.04092011]]</span><br></pre></td></tr></table></figure>
<p>可以自行对比一下直接用正规方程求解出的<span
class="math inline">\(\theta\)</span>与用线性回归模型训练出的<span
class="math inline">\(\theta\)</span>之间的差异，会"惊奇"地发现两者几乎一模一样。</p>
<h1 id="预测结果-1">7- 预测结果</h1>
<h3 id="任务11"><strong>任务11：</strong></h3>
<p>假设明天的最高温度是<span class="math inline">\(x_1 =
40\)</span>°C，人口<span class="math inline">\(x_2 =
3.3\)</span>百万，使用通过正规方程计算得到的<span
class="math inline">\(\theta\)</span>预测明天的城市的峰值用电量（单位：GW）吧！<br />
<strong>注意</strong>，<span
class="math inline">\(x\)</span>要进行同样的特征尺度归一化处理。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def predict(theta,x):</span><br><span class="line">    &quot;&quot;&quot;预测峰值用电量</span><br><span class="line">    参数：</span><br><span class="line">        X: 需要预测数据的特征,shape: (m, n+1), 这里只预测一天的结果, m=1</span><br><span class="line">        theta: 最终确定的参数,shape: (n+1, 1)</span><br><span class="line">    返回：</span><br><span class="line">        prediction: 预测结果,shape: (m, 1)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">### START CODE HERE ###</span><br><span class="line"></span><br><span class="line"># 零均值单位方差归一化</span><br><span class="line">x = (x-mu)/sigma</span><br><span class="line"># 在x前面加一列</span><br><span class="line">x = np.hstack([[[1]],x])</span><br><span class="line">#用theta和处理后的x计算预测值</span><br><span class="line">prediction = np.dot(theta.T,x.T)</span><br><span class="line">### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">return prediction</span><br><span class="line"></span><br><span class="line">#明天的特征</span><br><span class="line">x = np.array([[40,3.3]])</span><br><span class="line">print(&#x27;预计明天的峰值用电量为：%.2f GW&#x27;%(predict(theta,x))) </span><br></pre></td></tr></table></figure>
<p>结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">预计明天的峰值用电量为：4.25 GW</span><br></pre></td></tr></table></figure>
<h1 id="多项式回归">8- 多项式回归</h1>
<p>以上都是线性模型，当我们数据的特征<span
class="math inline">\(X\)</span>与预测结果<span
class="math inline">\(Y\)</span>之间没有明显的线性关系，而且又找不到合适的映射函数时，可以尝试多项式回归。
下面导入另一组最高气温与用电量数据，我们用线性模型试一试看看效果发现并不太好。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">data1 = np.loadtxt(&#x27;data1.txt&#x27;)</span><br><span class="line">X = data1[:,0].reshape(-1,1)</span><br><span class="line">Y = data1[:,1].reshape(-1,1)</span><br><span class="line"></span><br><span class="line">plt.scatter(X,Y)</span><br><span class="line">X = np.hstack((np.ones((X.shape[0],1)),X))</span><br><span class="line">theta = normal_equation(X,Y)</span><br><span class="line">plt.plot(np.sort(X[:,1]),np.dot(X,theta)[np.argsort(X[:,1])],&#x27;r&#x27;)</span><br></pre></td></tr></table></figure>
<p>多项式回归的最大优点就是可以通过增加<span
class="math inline">\(X\)</span>的高次项对实测点进行逼近，直至满意为止。事实上，多项式回归可以处理相当一类非线性问题，它在回归分析中占有重要的地位，<strong>因为任一函数都可以分段用多项式来逼近</strong>。因此，在通常的实际问题中，不论依变量与其他自变量的关系如何，我们总可以用多项式回归来进行分析。假设数据的特征只有一个<span
class="math inline">\(a\)</span>，多项式的最高次数为<span
class="math inline">\(K\)</span>，那么多项式回归方程为： <span
class="math display">\[
h(x) = \theta^T x = \theta_0 \times a^0 + \theta_1 \times a^1 + \theta_2
\times a^2 + \cdots + \theta_K \times a^K。
\]</span> 若令<span class="math inline">\(x = \begin{bmatrix} a^0, a^1,
a^2, \cdots, a^K \end{bmatrix}^T\)</span>，那么 <span
class="math display">\[
h(x) = \theta^T x = \theta_0 \times x_0 + \theta_1 \times x_1 + \theta_2
\times x_2, \cdots, \theta_K \times x_K，
\]</span> 这就变为多变量线性回归了。</p>
<h3 id="任务12"><strong>任务12：</strong></h3>
<p>现在想要得到一个如下的多项式模型，<span
class="math inline">\(K=2\)</span>，直接用上面的正规方程进行求解。 <span
class="math display">\[
    h(x) = \theta^T x = \theta_0 \times 1 + \theta_1 \times x + \theta_2
\times x^2。
\]</span> 输入数据<span class="math inline">\(X\)</span>变为: <span
class="math display">\[
\begin{bmatrix} x^{(0)} \\ x^{(1)} \\ \vdots \\x^{(m-1)}  \end{bmatrix}
\longrightarrow
\begin{bmatrix} 1\quad x^{(0)}\quad {x^{(0)}}^2 \\ 1\quad x^{(1)} \quad
{x^{(1)}}^2\\ \vdots \\ 1\ x^{(m-1)}\quad {x^{(m-1)}}^2  \end{bmatrix}。
\]</span> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">data1 = np.loadtxt(&#x27;data1.txt&#x27;)</span><br><span class="line">X = data1[:,0].reshape(-1,1)</span><br><span class="line">Y = data1[:,1].reshape(-1,1)</span><br><span class="line"></span><br><span class="line">m = X.shape[0]    # m 是数据X的行数</span><br><span class="line">X_square = np.power(X,2)</span><br><span class="line"></span><br><span class="line">### START CODE HERE ###</span><br><span class="line"></span><br><span class="line"># 对X 前面加1， 后面加平方，变为 m x 3 的矩阵</span><br><span class="line"></span><br><span class="line">X = np.hstack((np.ones((m,1)),X))</span><br><span class="line">X = np.hstack((X,X_square))</span><br><span class="line"></span><br><span class="line"># 用法线方程求解theta</span><br><span class="line"></span><br><span class="line">theta = normal_equation(X,Y)</span><br><span class="line"></span><br><span class="line">### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">plt.scatter(X[:,1],Y)</span><br><span class="line">plt.plot(np.sort(X[:,1]),np.dot(X,theta)[np.argsort(X[:,1])],&#x27;r&#x27;)</span><br></pre></td></tr></table></figure></p>
<p>所有任务到这里就结束了，下面是对上面的数据进行任意多项式拟合的结果，你可以通过改变<span
class="math inline">\(K\)</span>的值来调整多项式的阶数，看看不同模型的效果(但不设的太大,
<span class="math inline">\(K \le
193\)</span>)。可以看到，越复杂的模型，虽然拟合数据的效果越好，但是其泛化能力就会很差，所以模型的选择应该要尽量符合实际需求。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line">from sklearn.pipeline import Pipeline</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line"></span><br><span class="line">def PolynomialRegression(degree):</span><br><span class="line">    return Pipeline([</span><br><span class="line">        (&quot;poly&quot;,PolynomialFeatures(degree=degree)),</span><br><span class="line">        (&quot;std_scaler&quot;,StandardScaler()),</span><br><span class="line">        (&quot;lin_reg&quot;,LinearRegression())    </span><br><span class="line">    ])</span><br><span class="line">X = data1[:,0].reshape(-1,1)</span><br><span class="line">Y = data1[:,1].reshape(-1,1)</span><br><span class="line"></span><br><span class="line">K = 15  #可以调整K的值(0&lt;=K&lt;=193)</span><br><span class="line"></span><br><span class="line">poly_reg = PolynomialRegression(degree=K)</span><br><span class="line">poly_reg.fit(X,Y.squeeze())</span><br><span class="line">y_predict = poly_reg.predict(X)</span><br><span class="line">plt.scatter(X,Y)</span><br><span class="line">plt.plot(np.sort(X[:,0]),y_predict[np.argsort(X[:,0])],color=&#x27;r&#x27;)</span><br></pre></td></tr></table></figure>
<h2 id="学习调包sklearn-optional">学习调包sklearn (Optional)</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">plt.rcParams[&quot;font.sans-serif&quot;]=[&quot;SimHei&quot;] #设置字体</span><br><span class="line">plt.rcParams[&quot;axes.unicode_minus&quot;]=False #该语句解决图像中的“-”负号的乱码问题</span><br><span class="line"></span><br><span class="line">data = np.loadtxt(&quot;data.txt&quot;)</span><br><span class="line"># data 数据第一列为人口信息</span><br><span class="line">X_data = data[:, 0].reshape(-1,1)</span><br><span class="line"># data 数据第三列为城市峰值用电量</span><br><span class="line">y_data = data[:, 2].reshape(-1,1)</span><br><span class="line">print(&quot;X shape: &quot;, X_data.shape)</span><br><span class="line">print(&quot;y shape: &quot;, y_data.shape)</span><br></pre></td></tr></table></figure>
<p>结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X shape:  (80, 1)</span><br><span class="line">y shape:  (80, 1)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">linear_reg = LinearRegression()</span><br><span class="line"></span><br><span class="line">linear_reg.fit(X_data, y_data)</span><br><span class="line"></span><br><span class="line">X_test = np.array([[8], [45]])</span><br><span class="line">y_pred = linear_reg.predict(X_test)</span><br><span class="line">plt.plot(X_data,y_data,&quot;.&quot;)</span><br><span class="line">plt.plot(X_test, y_pred,&quot;r-&quot;)</span><br><span class="line">plt.xlabel(&quot;Temperture&quot;)</span><br><span class="line">plt.ylabel(&quot;PeakDemand&quot;)</span><br><span class="line">plt.title(&quot;Linear Regression model predictions&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/09/11/28/" rel="prev" title="刘梦熊：问题在经济 根子在政治">
      <i class="fa fa-chevron-left"></i> 刘梦熊：问题在经济 根子在政治
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">1.</span> <span class="nav-text">线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%90%86%E8%AE%BA%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.1.</span> <span class="nav-text">2- 单变量线性回归理论介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.1.1.</span> <span class="nav-text">单变量线性回归模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C"><span class="nav-number">1.1.2.</span> <span class="nav-text">预测结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-number">1.1.3.</span> <span class="nav-text">梯度下降法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.</span> <span class="nav-text">3- 实现单变量线性回归模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A11"><span class="nav-number">2.0.1.</span> <span class="nav-text">任务1：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A12"><span class="nav-number">2.0.2.</span> <span class="nav-text">任务2：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A13"><span class="nav-number">2.0.3.</span> <span class="nav-text">任务3：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A14"><span class="nav-number">2.0.4.</span> <span class="nav-text">任务4：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A15"><span class="nav-number">2.0.5.</span> <span class="nav-text">任务5：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A16"><span class="nav-number">2.0.6.</span> <span class="nav-text">任务6：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A17"><span class="nav-number">2.0.7.</span> <span class="nav-text">任务7：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.1.</span> <span class="nav-text">4- 实现多变量线性回归模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A18"><span class="nav-number">2.1.1.</span> <span class="nav-text">任务8：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">3.</span> <span class="nav-text">5- 特征归一化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A19"><span class="nav-number">3.0.1.</span> <span class="nav-text">任务9：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B3%95%E7%BA%BF%E6%96%B9%E7%A8%8B-the-normal-equations"><span class="nav-number">4.</span> <span class="nav-text">6- 法线方程 (The normal
equations)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A110"><span class="nav-number">4.0.1.</span> <span class="nav-text">任务10：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C-1"><span class="nav-number">5.</span> <span class="nav-text">7- 预测结果</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A111"><span class="nav-number">5.0.1.</span> <span class="nav-text">任务11：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92"><span class="nav-number">6.</span> <span class="nav-text">8- 多项式回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A112"><span class="nav-number">6.0.1.</span> <span class="nav-text">任务12：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E8%B0%83%E5%8C%85sklearn-optional"><span class="nav-number">6.1.</span> <span class="nav-text">学习调包sklearn (Optional)</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="赋 倖"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">赋 倖</p>
  <div class="site-description" itemprop="description">东京之北 千叶逢春 好景如水彩</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">29</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/SkyTravelar" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;SkyTravelar" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.shilinli.com/" title="http:&#x2F;&#x2F;www.shilinli.com&#x2F;" rel="noopener" target="_blank">indexss's</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">赋 倖</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
